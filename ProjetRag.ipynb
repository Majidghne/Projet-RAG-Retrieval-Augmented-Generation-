{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOzv01TrvyzSoZ9L1aDaa3E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Majidghne/Projet-RAG-Retrieval-Augmented-Generation-/blob/main/ProjetRag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oO-xT-9YFGvH"
      },
      "source": [
        "# **Projet RAG (Retrieval-Augmented Generation) avec Optimisation du Chunking**\n",
        "\n",
        "Ce notebook présente une implémentation de Retrieval-Augmented Generation (RAG) en utilisant ChromaDB comme base de données vectorielle et l'API d'OpenRouter pour la génération de texte. L'objectif principal est de démontrer l'importance de l'optimisation du *chunking* (découpage du texte) pour améliorer la pertinence des réponses du modèle. Nous explorerons différentes stratégies de *chunking* et leurs impacts sur les résultats.\n",
        "\n",
        "## **Étapes clés du projet :**\n",
        "\n",
        "1.  **Installation des Dépendances :** Mise en place de toutes les bibliothèques nécessaires.\n",
        "2.  **Lecture de Documents :** Fonctions pour lire différents formats de fichiers (PDF, DOCX, TXT).\n",
        "3.  **Ingestion des Données :** Téléchargement et traitement d'un document PDF dans ChromaDB.\n",
        "4.  **Implémentation RAG Initiale :** Démonstration d'un système RAG basique avec un *chunking* simple.\n",
        "5.  **Optimisation du Chunking :** Exploration de deux méthodes avancées (*sentence-based* et *LangChain RecursiveCharacterTextSplitter*) pour un découpage plus sémantique.\n",
        "6.  **Comparaison des Résultats :** Analyse de l'impact des différentes techniques de *chunking* sur la qualité des réponses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c072ecfb"
      },
      "source": [
        "## **1. Installation des Dépendances**\n",
        "\n",
        "Nous commençons par installer les bibliothèques Python nécessaires pour le traitement des documents, la création d'embeddings, la base de données vectorielle (ChromaDB) et l'accès à l'API d'OpenAI/OpenRouter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "tC3ZzHuk-u-W"
      },
      "outputs": [],
      "source": [
        "# Installer les bibliothèques nécessaires\n",
        "!pip install -qU chromadb # Base de données vectorielle pour le stockage des embeddings\n",
        "!pip install -qU openai # Accès à l'API OpenAI/OpenRouter pour les modèles de langage\n",
        "!pip install -qU pypdf2 # Pour la lecture et l'extraction de texte à partir de fichiers PDF\n",
        "!pip install -qU python-docx # Pour la lecture et l'extraction de texte à partir de fichiers Word\n",
        "!pip install -qU sentence-transformers # Pour la création d'embeddings (représentations vectorielles de texte)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e56b349f"
      },
      "source": [
        "## **2. Fonctions de Lecture de Documents**\n",
        "\n",
        "Ces fonctions permettent de lire le contenu textuel de différents types de fichiers (PDF, Word, TXT). Une fonction unifiée `read_document` gère la détection du format et l'appel de la fonction de lecture appropriée."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2 # Importe la bibliothèque pour la lecture de fichiers PDF\n",
        "import docx # Importe la bibliothèque pour la lecture de fichiers DOCX\n",
        "import os # Importe le module os pour les opérations sur les chemins de fichiers\n",
        "\n",
        "def read_text_file(file_path: str):\n",
        "    \"\"\"Lire le contenu d'un fichier texte\"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        return file.read()\n",
        "\n",
        "def read_pdf_file(file_path: str):\n",
        "  \"\"\"Lire le contenu d'un fichier PDF\"\"\"\n",
        "  text=\"\"\n",
        "  with open(file_path, 'rb') as file: # Ouvre le fichier PDF en mode binaire\n",
        "    pdf_reader=PyPDF2.PdfReader(file) # Crée un objet PdfReader\n",
        "    for page in pdf_reader.pages: # Itère sur chaque page du PDF\n",
        "      text += page.extract_text() + \"\\n\" # Extrait le texte de la page et l'ajoute\n",
        "  return text\n",
        "\n",
        "def read_docx_file(file_path: str):\n",
        "  \"\"\"Lire le contenu d'un fichier Word\"\"\"\n",
        "  doc = docx.Document(file_path) # Ouvre le document Word\n",
        "  return \"\\n\".join([paragraph.text for paragraph in doc.paragraphs]) # Extrait le texte de chaque paragraphe et le joint"
      ],
      "metadata": {
        "collapsed": true,
        "id": "VDkbtzpIAbCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_document(file_path: str):\n",
        "    \"\"\"Lit le contenu d'un document en fonction de son extension de fichier.\"\"\"\n",
        "    _, file_extension = os.path.splitext(file_path) # Extrait l'extension du fichier\n",
        "    file_extension = file_extension.lower() # Convertit l'extension en minuscules\n",
        "    if file_extension == \".txt\":\n",
        "        return read_text_file(file_path)\n",
        "    elif file_extension == \".pdf\":\n",
        "        return read_pdf_file(file_path)\n",
        "    elif file_extension == \".docx\":\n",
        "        return read_docx_file(file_path)\n",
        "    else:\n",
        "        raise ValueError(f\"Format de fichier non supporté: {file_extension}\")"
      ],
      "metadata": {
        "id": "STdFrfmXNuMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cdc69ff"
      },
      "source": [
        "## **3. Téléchargement et Traitement du Document**\n",
        "\n",
        "Nous téléchargeons un document PDF à partir de Google Drive. Ce document servira de source de connaissances pour notre système RAG. Nous le lisons et affichons un extrait pour vérification."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Télécharge le fichier PDF depuis Google Drive\n",
        "!gdown \"https://drive.google.com/uc?id=1TywvYowEeL49qd-HKjr9-RUzjexz0N94\""
      ],
      "metadata": {
        "id": "u67EnYf37Xdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lister les fichiers pdf dans /content\n",
        "files = [f for f in os.listdir(\"/content\") if f.endswith(\".pdf\")] # Filtre les fichiers pour ne garder que les PDF"
      ],
      "metadata": {
        "id": "J1f_87wNX2KR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Construit le chemin complet du dernier fichier PDF trouvé\n",
        "file_path = os.path.join(\"/content\", files[-1])\n",
        "\n",
        "print(file_path)\n",
        "\n",
        "text = read_document(file_path) # Lit le contenu du document PDF\n",
        "\n",
        "print(\"\\n======Extracted PDF Content=======\\n\")\n",
        "print(text[:500]) # Affiche les 500 premiers caractères du texte extrait pour vérification"
      ],
      "metadata": {
        "id": "OfkiKDW5UAdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fc00547"
      },
      "source": [
        "## **4. Implémentation RAG Initiale : Chunking Basique**\n",
        "\n",
        "Dans cette première approche, nous utilisons une méthode de *chunking* simple, basée sur une taille de morceau fixe avec un chevauchement. Nous définissons également les fonctions pour la recherche sémantique avec ChromaDB et la génération de réponses via l'API OpenRouter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d0b652b"
      },
      "source": [
        "### **Fonction de Chunking Basique (`split_text`)**\n",
        "\n",
        "Cette fonction découpe le texte en morceaux de taille fixe (500 caractères par défaut) avec un chevauchement (100 caractères par défaut). Cette méthode est simple mais peut souvent entraîner une perte de cohérence sémantique en coupant les phrases ou les idées au milieu."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_text(text: str, chunk_size: int = 500, chunk_overlap: int = 100):\n",
        "  \"\"\"Sépare le texte en morceaux plus petits avec une taille fixe et un chevauchement.\"\"\"\n",
        "  text = text.replace(\"\\n\", \" \").strip() # Remplace les sauts de ligne par des espaces et nettoie le texte\n",
        "  chunks = []\n",
        "  start = 0\n",
        "  length = len(text)\n",
        "\n",
        "  while start < length:\n",
        "        end = min(start + chunk_size, length) # Détermine la fin du morceau, sans dépasser la longueur du texte\n",
        "        chunk = text[start:end].strip() # Extrait le morceau\n",
        "        if chunk:\n",
        "            chunks.append(chunk) # Ajoute le morceau si non vide\n",
        "        start += chunk_size - chunk_overlap # Avance le début du prochain morceau en considérant le chevauchement\n",
        "\n",
        "  return chunks"
      ],
      "metadata": {
        "id": "5D4dcioU8A0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemple de découpage avec la fonction split_text\n",
        "sample = \"This is a very long paragraph of text that you want to split into smaller chunks for embedding or storage.\"\n",
        "chunks = split_text(sample, chunk_size=10, chunk_overlap=2) # Teste avec une petite taille de morceau\n",
        "print(chunks)"
      ],
      "metadata": {
        "id": "ekKIJ03_L1is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applique le chunking basique au texte complet du document\n",
        "chunks = split_text(text, chunk_size=500, chunk_overlap=50)\n",
        "\n",
        "print(\"Chunk-01\", chunks[0]) # Affiche le premier morceau\n",
        "print(\"Chunk-02\", chunks[1]) # Affiche le deuxième morceau\n",
        "print(\"Number of Chunks\", len(chunks)) # Affiche le nombre total de morceaux"
      ],
      "metadata": {
        "id": "Iejo-PUlMxJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3939cf54"
      },
      "source": [
        "### **Initialisation de ChromaDB**\n",
        "\n",
        "Nous configurons notre base de données vectorielle persistante (ChromaDB) et définissons une fonction d'embedding (`all-MiniLM-L6-v2`) pour transformer nos morceaux de texte en vecteurs numériques."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb # Importe la bibliothèque ChromaDB\n",
        "\n",
        "from chromadb.utils import embedding_functions # Importe les fonctions d'embedding de ChromaDB\n",
        "\n",
        "client = chromadb.PersistentClient(path=\"chroma_db\") # Initialisation d'un client ChromaDB persistant qui stocke les données localement dans le dossier \"chroma_db\"\n",
        "\n",
        "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
        "    model_name=\"all-MiniLM-L6-v2\" # Utilise le modèle \"all-MiniLM-L6-v2\" pour créer les embeddings\n",
        ")\n",
        "\n",
        "collection = client.get_or_create_collection(\n",
        "    name=\"documents_collection\", # Nom de la collection où les documents seront stockés\n",
        "    embedding_function=sentence_transformer_ef # Fonction d'embedding à utiliser pour cette collection\n",
        ")"
      ],
      "metadata": {
        "id": "I5haj8GyOdkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf2d1b53"
      },
      "source": [
        "### **Traitement du Document pour ChromaDB**\n",
        "\n",
        "La fonction `process_document` combine la lecture, le *chunking* et la préparation des métadonnées (source, numéro de morceau) pour l'ingestion dans ChromaDB."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_document(file_path: str):\n",
        "  \"\"\"Traite un document en le lisant, le découpant (chunking) et préparant les métadonnées pour ChromaDB.\"\"\"\n",
        "  try:\n",
        "        content = read_document(file_path) # Lit le contenu du document\n",
        "\n",
        "        chunks = split_text(content) # Découpe le contenu en morceaux en utilisant la fonction de chunking basique\n",
        "\n",
        "        file_name = os.path.basename(file_path) # Extrait le nom du fichier\n",
        "        metadatas = [{\"source\": file_name, \"chunk\": i} for i in range(len(chunks))] # Crée des métadonnées pour chaque morceau (source et numéro de morceau)\n",
        "        ids = [f\"{file_name}_chunk_{i}\" for i in range(len(chunks))] # Génère des IDs uniques pour chaque morceau\n",
        "\n",
        "        return ids, chunks, metadatas # Retourne les IDs, les morceaux et leurs métadonnées\n",
        "  except Exception as e:\n",
        "        print(f\"Error processing {file_path}: {str(e)}\")\n",
        "        return [], [], []"
      ],
      "metadata": {
        "id": "7CdARt9bRHoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b25035d7"
      },
      "source": [
        "### **Vérification des Chunks Initiaux**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Traite le document pour obtenir les IDs, les chunks et les métadonnées\n",
        "ids, chunks, metadatas = process_document(file_path)"
      ],
      "metadata": {
        "id": "r8B5931yWSVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Affiche les informations du premier chunk pour vérification\n",
        "print(\"id[0] -> \", ids[0])\n",
        "print(\"metadatas[0] -> \", metadatas[0])\n",
        "print(\"chunks[0] -> \", chunks[0])"
      ],
      "metadata": {
        "id": "js4XSgJvWa2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Affiche le nombre total de chunks générés\n",
        "len(chunks)"
      ],
      "metadata": {
        "id": "CmLC7rpGWel6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c67221a6"
      },
      "source": [
        "### **Ajout des Chunks à ChromaDB**\n",
        "\n",
        "Les morceaux de texte sont maintenant convertis en embeddings et stockés dans la collection ChromaDB, prêts pour la recherche sémantique."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ajoute les documents (chunks), les métadonnées et les IDs à la collection ChromaDB\n",
        "collection.add(documents=chunks, metadatas=metadatas, ids=ids)"
      ],
      "metadata": {
        "id": "lsa5gPE_WiEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "041b6ee4"
      },
      "source": [
        "### **Fonctions de Recherche Sémantique et de RAG**\n",
        "\n",
        "Ces fonctions gèrent la recherche des morceaux les plus pertinents dans ChromaDB (`semantic_search`), la construction du contexte à partir de ces morceaux (`get_context_with_sources`), et l'appel au modèle de langage pour générer une réponse (`rag_answer`)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def semantic_search(collection, query: str, n_results: int = 2):\n",
        "    \"\"\"Effectue une recherche sémantique dans la collection ChromaDB.\"\"\"\n",
        "    return collection.query(\n",
        "        query_texts=[query], # La requête de recherche\n",
        "        n_results=n_results, # Nombre de résultats les plus pertinents à retourner\n",
        "        include=[\"documents\", \"metadatas\"] # Inclut le contenu des documents et leurs métadonnées dans les résultats\n",
        "    )\n",
        "\n",
        "def get_context_with_sources(results):\n",
        "    \"\"\"Extrait le contexte et les sources des résultats de recherche.\"\"\"\n",
        "    if not results or not results.get(\"documents\") or not results[\"documents\"][0]:\n",
        "        return \"\", []\n",
        "\n",
        "    context = \"\\n\\n\".join(results[\"documents\"][0]) # Concatène les morceaux de document trouvés pour former le contexte\n",
        "\n",
        "    seen = set()\n",
        "    sources = []\n",
        "    for meta in results[\"metadatas\"][0]:\n",
        "        label = f\"{meta.get('source','?')} (chunk {meta.get('chunk','?')})\" # Formate l'étiquette de la source\n",
        "        if label not in seen:\n",
        "            seen.add(label)\n",
        "            sources.append(label) # Ajoute la source si elle n'a pas déjà été vue\n",
        "\n",
        "    return context, sources\n",
        "\n",
        "def ask(collection, query: str, n_results: int = 2):\n",
        "    \"\"\"Fonction utilitaire pour effectuer une recherche, construire le contexte et afficher les sources.\"\"\"\n",
        "\n",
        "    results = semantic_search(collection, query, n_results=n_results)\n",
        "\n",
        "    context, sources = get_context_with_sources(results)\n",
        "\n",
        "    print(\"\\n=== CONTEXT ===\\n\")\n",
        "    print(context or \"[No matching text found]\")\n",
        "\n",
        "    print(\"\\n=== SOURCES ===\")\n",
        "    if sources:\n",
        "        for i, s in enumerate(sources, 1):\n",
        "            print(f\"{i}. {s}\")\n",
        "    else:\n",
        "        print(\"[No sources]\")\n",
        "\n",
        "    return context, sources"
      ],
      "metadata": {
        "id": "SAUbbqBW8zkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemple de question posée avec le chunking basique\n",
        "query = \"Quelle est un système fermé?\"\n",
        "context, sources = ask(collection, query, n_results=5) # Effectue la recherche et affiche le contexte et les sources"
      ],
      "metadata": {
        "id": "u6OleD3ICVLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "510e145d"
      },
      "source": [
        "### **Configuration de l'API OpenRouter**\n",
        "\n",
        "Nous utilisons OpenRouter pour accéder à des modèles de langage, en configurant la clé API et le nom du modèle."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv # Importe load_dotenv pour charger les variables d'environnement\n",
        "\n",
        "# Crée ou écrase un fichier .env avec une clé API placeholder\n",
        "with open(\".env\", \"w\") as f:\n",
        "    f.write('OPEN_ROUTER_API_KEY=\"VOTRE_CLE_ICI\"') # Vous devez remplacer \"VOTRE_CLE_ICI\" par votre vraie clé\n",
        "\n",
        "load_dotenv() # Charge les variables d'environnement depuis le fichier .env"
      ],
      "metadata": {
        "id": "B2xlWUSp7O_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OPEN_ROUTER_API_KEY = os.getenv(\"OPEN_ROUTER_API_KEY\") # Récupère la clé API depuis les variables d'environnement\n",
        "OPEN_ROUTER_MODEL_NAME = \"openai/gpt-oss-120b:free\" # Définit le nom du modèle OpenRouter à utiliser"
      ],
      "metadata": {
        "id": "bzegGzAJ_Bar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI # Importe la classe OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "  base_url = \"https://openrouter.ai/api/v1\", # Définit l'URL de base pour l'API OpenRouter\n",
        "  api_key = OPEN_ROUTER_API_KEY, # Utilise la clé API configurée\n",
        ")"
      ],
      "metadata": {
        "id": "72ugvRPbANal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = (\n",
        "    \"You are a helpful assistant for retrieval-augmented generation (RAG).\\n\" # Rôle de l'assistant\n",
        "    \"Answer ONLY using the provided context. \" # Instruction cruciale: ne répondre qu'avec le contexte fourni\n",
        "    \"If the answer is not found in the context, say: \"\n",
        "    \"'I don't know based on the provided documents.'\" # Réponse si l'information n'est pas dans le contexte\n",
        ")\n",
        "\n",
        "def build_messages(context: str, question: str):\n",
        "    \"\"\"Construit la liste des messages pour l'API OpenAI/OpenRouter.\"\"\"\n",
        "    return [\n",
        "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT}, # Message système pour définir le comportement de l'IA\n",
        "        {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {question}\\nAnswer:\"} # Message utilisateur incluant le contexte et la question\n",
        "    ]"
      ],
      "metadata": {
        "id": "oW3ImeV6Bq6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rag_answer(collection, query: str, n_results: int = 4, model: str = OPEN_ROUTER_MODEL_NAME):\n",
        "    \"\"\"Exécute la recherche sémantique, génère une réponse avec l'IA et affiche les résultats.\"\"\"\n",
        "    results = semantic_search(collection, query, n_results) # Effectue la recherche sémantique\n",
        "    context, sources = get_context_with_sources(results) # Extrait le contexte et les sources\n",
        "\n",
        "    if not context.strip(): # Si aucun contexte pertinent n'est trouvé\n",
        "        print(\"No relevant context found.\")\n",
        "        return \"\", []\n",
        "\n",
        "    messages = build_messages(context, query) # Construit les messages pour l'API de l'IA\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0.2, # Température basse pour des réponses plus déterministes\n",
        "        max_tokens=512 # Limite la longueur de la réponse\n",
        "    )\n",
        "\n",
        "    answer = response.choices[0].message.content.strip() # Extrait le contenu de la réponse de l'IA\n",
        "\n",
        "    print(\"\\n=== ANSWER ===\\n\")\n",
        "    print(answer or \"[No answer generated]\")\n",
        "\n",
        "    print(\"\\n=== SOURCES ===\")\n",
        "    if sources:\n",
        "        for i, s in enumerate(sources, 1):\n",
        "            print(f\"{i}. {s}\")\n",
        "    else:\n",
        "        print(\"[No sources found]\")\n",
        "\n",
        "    return answer, sources"
      ],
      "metadata": {
        "id": "ymjyQMtUEywW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "451b101f"
      },
      "source": [
        "### **Exemple de Réponse RAG (Chunking Basique)**\n",
        "\n",
        "Nous posons une question et observons la réponse générée avec le *chunking* basique. Notez les sources utilisées."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemple de question pour tester le RAG avec le chunking basique\n",
        "query = \"Cycles moteur à vapeur\"\n",
        "rag_answer(collection, query, n_results=2)"
      ],
      "metadata": {
        "id": "s_06MfAhGnfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Autre exemple de question avec le chunking basique\n",
        "query = \"cycle de Diesel?\"\n",
        "rag_answer(collection, query, n_results=3)"
      ],
      "metadata": {
        "id": "aXLjUQo6GqJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Troisième exemple de question avec le chunking basique\n",
        "query = \"Quelle est une évolution rapide?\"\n",
        "rag_answer(collection, query, n_results=5)"
      ],
      "metadata": {
        "id": "0meb9lFoGtLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "279a5283"
      },
      "source": [
        "## **5. Optimisation du Chunking**\n",
        "\n",
        "Le *chunking* basique peut entraîner des problèmes de cohérence sémantique :\n",
        "* Coupe au milieu d’une phrase\n",
        "* Peut séparer une définition en deux parties\n",
        "* Peut séparer une question et sa réponse\n",
        "* Entraîne une perte de cohérence sémantique\n",
        "\n",
        " Par conséquent, les embeddings sont moins pertinents et la recherche devient moins précise.\n",
        "\n",
        "Alors, nous allons explorer deux solutions pour améliorer la qualité du découpage des textes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d390d346"
      },
      "source": [
        "### **Solution 1 : Chunking basé sur les phrases (NLTK)**\n",
        "\n",
        "Cette méthode vise à conserver l'intégrité des phrases. Nous utilisons la bibliothèque `NLTK` pour la tokenisation des phrases, puis nous construisons des morceaux en regroupant des phrases jusqu'à atteindre une taille maximale. Cela réduit le risque de couper des informations importantes au milieu d'une phrase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cccbfa3c"
      },
      "source": [
        "### **Installation de NLTK**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installe la bibliothèque NLTK\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt') # Télécharge le tokenizer 'punkt' pour le découpage de phrases\n",
        "nltk.download('punkt_tab') # Télécharge une version tabulaire du tokenizer (si nécessaire)\n",
        "from nltk.tokenize import sent_tokenize # Importe la fonction de tokenization de phrases"
      ],
      "metadata": {
        "id": "bFdEGXXP43dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f2128ac"
      },
      "source": [
        "### **Fonction de Chunking par Phrases (`split_text_by_sentences`)**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_text_by_sentences(text, max_chunk_size=500):\n",
        "    \"\"\"Découpe le texte en morceaux en respectant les limites de phrases.\"\"\"\n",
        "    sentences = sent_tokenize(text) # Découpe le texte en phrases\n",
        "\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "\n",
        "    for sentence in sentences:\n",
        "        # Si ajouter la phrase actuelle ne dépasse pas la taille maximale du morceau\n",
        "        if len(current_chunk) + len(sentence) <= max_chunk_size:\n",
        "            current_chunk += \" \" + sentence # Ajoute la phrase au morceau courant\n",
        "        else:\n",
        "            chunks.append(current_chunk.strip()) # Ajoute le morceau courant aux chunks\n",
        "            current_chunk = sentence # Commence un nouveau morceau avec la phrase actuelle\n",
        "\n",
        "    if current_chunk: # Ajoute le dernier morceau s'il n'est pas vide\n",
        "        chunks.append(current_chunk.strip())\n",
        "\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "mviOEdyJ4_iI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66dac19a"
      },
      "source": [
        "### **Application et Vérification des Nouveaux Chunks**\n",
        "\n",
        "Nous appliquons cette nouvelle méthode de *chunking* au texte du document."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Applique le découpage par phrases au texte du document\n",
        "chunks = split_text_by_sentences(text)\n",
        "print(\"Nombre de chunks :\", len(chunks)) # Affiche le nombre de chunks après découpage par phrases"
      ],
      "metadata": {
        "id": "2n8bhRs55APw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db4130f9"
      },
      "source": [
        "### **Réinitialisation et Rechargement de la Collection ChromaDB**\n",
        "\n",
        "Pour comparer les résultats, nous devons d'abord vider l'ancienne collection ChromaDB et la re-remplir avec les nouveaux chunks obtenus par la méthode basée sur les phrases. Nous allons d'abord supprimer la collection existante si elle existe, puis la recréer. **Note:** Dans cet exemple, les étapes `ids, chunks, metadatas = process_document(file_path)` et `collection.add(...)` doivent être relancées avec la nouvelle fonction de chunking pour refléter les changements."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_document2(file_path: str):\n",
        "  \"\"\"Traite un document en utilisant le chunking basé sur les phrases pour ChromaDB.\"\"\"\n",
        "  try:\n",
        "        content = read_document(file_path) # Lit le contenu du document\n",
        "\n",
        "        chunks = split_text_by_sentences(content) # Utilise la nouvelle fonction de chunking par phrases\n",
        "\n",
        "        file_name = os.path.basename(file_path) # Extrait le nom du fichier\n",
        "        metadatas = [{\"source\": file_name, \"chunk\": i} for i in range(len(chunks))] # Crée les métadonnées\n",
        "        ids = [f\"{file_name}_chunk_{i}\" for i in range(len(chunks))] # Génère les IDs\n",
        "\n",
        "        return ids, chunks, metadatas\n",
        "  except Exception as e:\n",
        "        print(f\"Error processing {file_path}: {str(e)}\")\n",
        "        return [], [], []\n"
      ],
      "metadata": {
        "id": "S0MLzKfFLqJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Traite le document avec le chunking basé sur les phrases\n",
        "ids, chunks, metadatas = process_document2(file_path)"
      ],
      "metadata": {
        "id": "Sy9zRJ6OC4jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ajoute les nouveaux chunks (basés sur les phrases) à la collection ChromaDB. Note: cela va s'ajouter aux chunks existants si la collection n'a pas été vidée.\n",
        "collection.add(documents=chunks, metadatas=metadatas, ids=ids)"
      ],
      "metadata": {
        "id": "97hNWkkU7RoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4607b315"
      },
      "source": [
        "### **Exemple de Réponse RAG (Chunking NLTK)**\n",
        "\n",
        "Nous posons la même question et observons la réponse générée avec le *chunking* basé sur les phrases. Comparons la qualité des réponses et des sources avec la méthode précédente."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pose la même question avec les chunks basés sur les phrases\n",
        "query = \"Cycles moteur à vapeur\"\n",
        "rag_answer(collection, query, n_results=2)"
      ],
      "metadata": {
        "id": "5sILYBvD7xk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Autre question avec les chunks basés sur les phrases\n",
        "query = \"cycle de Diesel?\"\n",
        "rag_answer(collection, query, n_results=3)"
      ],
      "metadata": {
        "id": "_5dsPH1Q7RoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Troisième question avec les chunks basés sur les phrases\n",
        "query = \"Quelle est une évolution rapide?\"\n",
        "rag_answer(collection, query, n_results=5)"
      ],
      "metadata": {
        "id": "tZaia9BP7RoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d16948aa"
      },
      "source": [
        "### **Solution 2 : Chunking avancé avec LangChain `RecursiveCharacterTextSplitter`**\n",
        "\n",
        "LangChain est une bibliothèque Python conçue pour construire des applications avec des LLM :\n",
        "\n",
        "* RAG (Retrieval-Augmented Generation)\n",
        "* Agents\n",
        "* Chatbots avec mémoire\n",
        "* Pipelines LLM complexes\n",
        "\n",
        "Dans notre cas, on l’utiliserait uniquement pour faire un meilleur découpage du texte (text splitting).\n",
        "\n",
        "**`RecursiveCharacterTextSplitter`**\n",
        "\n",
        " Cet outil tente de découper le texte de manière hiérarchique en utilisant une liste de séparateurs, ce qui permet de préserver au maximum la structure sémantique du document."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a593a48"
      },
      "source": [
        "### **Installation des Dépendances LangChain**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installe les bibliothèques LangChain nécessaires\n",
        "!pip install -q langchain langchain-text-splitters"
      ],
      "metadata": {
        "id": "GuLXLvzz-J2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da61c498"
      },
      "source": [
        "### **Application et Vérification des Chunks avec LangChain**\n",
        "\n",
        "Nous appliquons `RecursiveCharacterTextSplitter` au texte de notre document."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter # Importe le découpeur de texte récursif de LangChain\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500, # Taille maximale souhaitée pour chaque morceau\n",
        "    chunk_overlap=50, # Chevauchement entre les morceaux pour préserver le contexte\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"] # Ordre des séparateurs à essayer pour le découpage (paragraphe, ligne, phrase, mot, caractère)\n",
        ")\n",
        "\n",
        "chunks = text_splitter.split_text(text) # Applique le découpeur au texte complet\n",
        "\n",
        "print(\"Nombre de chunks :\", len(chunks))\n",
        "print(\"Chunk 1:\\n\", chunks[0])"
      ],
      "metadata": {
        "id": "L2eIdnP-Aolz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "873a7aeb"
      },
      "source": [
        "### **Réinitialisation et Rechargement de la Collection ChromaDB (avec LangChain Chunks)**\n",
        "\n",
        "Nous allons maintenant vider la collection ChromaDB et la re-remplir avec les chunks générés par LangChain."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_document3(file_path: str):\n",
        "  \"\"\"Traite un document en utilisant le `RecursiveCharacterTextSplitter` de LangChain pour ChromaDB.\"\"\"\n",
        "  try:\n",
        "        content = read_document(file_path) # Lit le contenu du document\n",
        "\n",
        "        chunks = text_splitter.split_text(content) # Utilise la fonction de chunking de LangChain\n",
        "\n",
        "        file_name = os.path.basename(file_path) # Extrait le nom du fichier\n",
        "        metadatas = [{\"source\": file_name, \"chunk\": i} for i in range(len(chunks))] # Crée les métadonnées\n",
        "        ids = [f\"{file_name}_chunk_{i}\" for i in range(len(chunks))] # Génère les IDs\n",
        "\n",
        "        return ids, chunks, metadatas\n",
        "  except Exception as e:\n",
        "        print(f\"Error processing {file_path}: {str(e)}\")\n",
        "        return [], [], []\n"
      ],
      "metadata": {
        "id": "5u_hBKhEMNVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Traite le document avec le chunking LangChain\n",
        "ids, chunks, metadatas = process_document3(file_path)"
      ],
      "metadata": {
        "id": "BfrBZdIKCT27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ajoute les nouveaux chunks (basés sur LangChain) à la collection ChromaDB\n",
        "collection.add(documents=chunks, metadatas=metadatas, ids=ids)"
      ],
      "metadata": {
        "id": "R8te0iOHB9vy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e98c602"
      },
      "source": [
        "### **Exemple de Réponse RAG (Chunking LangChain)**\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pose la question avec les chunks générés par LangChain\n",
        "query = \"Cycles moteur à vapeur\"\n",
        "rag_answer(collection, query, n_results=2)"
      ],
      "metadata": {
        "id": "VfEzfXYPB9vz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Autre question avec les chunks générés par LangChain\n",
        "query = \"cycle de Diesel?\"\n",
        "rag_answer(collection, query, n_results=3)"
      ],
      "metadata": {
        "id": "ycJUc9_8DJC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Troisième question avec les chunks générés par LangChain\n",
        "query = \"Quelle est une évolution rapide?\"\n",
        "rag_answer(collection, query, n_results=5)"
      ],
      "metadata": {
        "id": "W1NYcoi0B9v0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43453f9d"
      },
      "source": [
        "## **Conclusion : L'Importance du Chunking**\n",
        "\n",
        "À travers ces exemples, nous avons pu observer comment différentes stratégies de *chunking* peuvent influencer la qualité des réponses d'un système RAG. Un découpage intelligent du texte, qui respecte la cohérence sémantique, est crucial pour obtenir des embeddings plus pertinents et, par conséquent, des recherches plus précises et des réponses plus fidèles aux documents sources.\n",
        "\n",
        "Le `RecursiveCharacterTextSplitter` de LangChain, avec sa stratégie hiérarchique de découpage, offre souvent les meilleurs résultats en équilibrant la taille des *chunks* et la préservation du contexte sémantique. Cela démontre que l'ingénierie du *chunking* est une étape fondamentale pour construire des applications RAG performantes et fiables."
      ]
    }
  ]
}